{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run the instructions in here - [Accessing AWS](https://www.notion.so/lattice/Accessing-AWS-043c558c833044e38329ef3dc9c1ea26#4e7d707777864670814e37aef0a08a6b) which will log you in using your okta roles. this will last 4 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred: Credentials were refreshed, but the refreshed credentials are still expired.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SCRIPT TO GET MESSAGES FROM QUEUE AND WRITE TO CSV\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import csv\n",
    "import json\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "\n",
    "def flatten_json(y, parent_key='', sep='.'):\n",
    "    \"\"\"\n",
    "    Flattens a nested JSON object.\n",
    "\n",
    "    :param y: The JSON object to flatten.\n",
    "    :param parent_key: The base key string.\n",
    "    :param sep: Separator between keys.\n",
    "    :return: A flattened dictionary.\n",
    "    \"\"\"\n",
    "    items = {}\n",
    "    for k, v in y.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.update(flatten_json(v, new_key, sep=sep))\n",
    "        elif isinstance(v, list):\n",
    "            # Handle lists by joining items with a separator or indexing\n",
    "            items[new_key] = '; '.join(map(str, v))\n",
    "        else:\n",
    "            items[new_key] = v\n",
    "    return items\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Create SQS client using the session\n",
    "        sqs = boto3.client('sqs')\n",
    "\n",
    "        # Get the URL for the SQS queue\n",
    "        queue_url = sqs.get_queue_url(QueueName='weaver-worker-text-analysis-dlq')['QueueUrl']\n",
    "        # queue_url = sqs.get_queue_url(QueueName='text-analysis-clustering-dlq')['QueueUrl']\n",
    "\n",
    "        all_messages = []\n",
    "        csv_headers = set(['MessageId', 'ReceiptHandle'])  # Initial headers\n",
    "\n",
    "        # Define maximum number of messages to prevent infinite loops (optional)\n",
    "        MAX_MESSAGES = 10000  # Adjust as needed\n",
    "        fetched_messages = 0\n",
    "\n",
    "        while True:\n",
    "            # Receive messages from SQS queue\n",
    "            response = sqs.receive_message(\n",
    "                QueueUrl=queue_url,\n",
    "                MaxNumberOfMessages=10,  # Maximum allowed by SQS\n",
    "                VisibilityTimeout=30,    # Adjust based on processing time\n",
    "                WaitTimeSeconds=10       # Long polling\n",
    "            )\n",
    "\n",
    "            messages = response.get('Messages', [])\n",
    "\n",
    "            if not messages:\n",
    "                print(\"No more messages available in the queue.\")\n",
    "                break\n",
    "\n",
    "            for message in messages:\n",
    "                message_id = message.get('MessageId', '')\n",
    "                receipt_handle = message.get('ReceiptHandle', '')\n",
    "                body = message.get('Body', '{}')  # Default to empty JSON if Body is missing\n",
    "\n",
    "                try:\n",
    "                    # Parse the JSON body\n",
    "                    body_json = json.loads(body)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON for MessageId {message_id}: {e}\")\n",
    "                    body_json = {}\n",
    "\n",
    "                # Flatten the JSON body\n",
    "                flattened_body = flatten_json(body_json)\n",
    "\n",
    "                # Update headers with keys from the flattened JSON\n",
    "                csv_headers.update(flattened_body.keys())\n",
    "\n",
    "                # Combine message metadata with flattened body\n",
    "                combined_message = {\n",
    "                    'MessageId': message_id,\n",
    "                    'ReceiptHandle': receipt_handle\n",
    "                }\n",
    "                combined_message.update(flattened_body)\n",
    "\n",
    "                all_messages.append(combined_message)\n",
    "\n",
    "                should_delete = input(\"Delete message from SQS? (y/n): \")\n",
    "                if should_delete == \"y\":\n",
    "                    # Optional: Delete the message after processing\n",
    "                    # Uncomment the following lines if you want to delete messages\n",
    "                    sqs.delete_message(\n",
    "                        QueueUrl=queue_url,\n",
    "                        ReceiptHandle=receipt_handle\n",
    "                    )\n",
    "\n",
    "                fetched_messages += 1\n",
    "\n",
    "                # Check if maximum message limit is reached\n",
    "                if fetched_messages >= MAX_MESSAGES:\n",
    "                    print(f\"Reached maximum limit of {MAX_MESSAGES} messages.\")\n",
    "                    break\n",
    "\n",
    "            # Optional: Print progress\n",
    "            print(f\"Fetched {fetched_messages} messages so far.\")\n",
    "\n",
    "            # Exit if maximum message limit is reached\n",
    "            if fetched_messages >= MAX_MESSAGES:\n",
    "                break\n",
    "\n",
    "        if not all_messages:\n",
    "            print(\"No messages were fetched from the queue.\")\n",
    "            return\n",
    "\n",
    "        # Define the order of CSV headers (optional)\n",
    "        ordered_headers = ['MessageId', 'ReceiptHandle'] + sorted(k for k in csv_headers if k not in ['MessageId', 'ReceiptHandle'])\n",
    "\n",
    "        # Write CSV data to a file\n",
    "        date = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "        with open(f'sqs_messages-{date}.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=ordered_headers)\n",
    "            writer.writeheader()\n",
    "            for msg in all_messages:\n",
    "                writer.writerow(msg)\n",
    "\n",
    "        print(f\"{len(all_messages)} messages have been written to sqs_messages.csv\")\n",
    "\n",
    "    except NoCredentialsError:\n",
    "        print(\"Error: AWS credentials not found. Please authenticate using ktool.\")\n",
    "    except PartialCredentialsError:\n",
    "        print(\"Error: Incomplete AWS credentials. Please check your ktool configuration.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        sqs = boto3.client('sqs')\n",
    "\n",
    "        # Prompt user for MessageIds to redrive\n",
    "        message_ids_input = input(\"Enter comma-separated MessageIds to redrive: \")\n",
    "        message_ids_to_redrive = set(mid.strip() for mid in message_ids_input.split(',') if mid.strip())\n",
    "        if not message_ids_to_redrive:\n",
    "            print(\"No MessageIds provided. Exiting.\")\n",
    "            return\n",
    "\n",
    "        # Get DLQ URL and original queue URL\n",
    "        dlq_name = 'weaver-worker-ai-integrations-dlq'\n",
    "        queue_url = sqs.get_queue_url(QueueName=dlq_name)['QueueUrl']\n",
    "        orig_queue_name = dlq_name[:-4] if dlq_name.endswith('-dlq') else dlq_name  # Remove '-dlq'\n",
    "        orig_queue_url = sqs.get_queue_url(QueueName=orig_queue_name)['QueueUrl']\n",
    "\n",
    "        matched_messages = []\n",
    "        fetched_messages = 0\n",
    "        MAX_MESSAGES = 10000\n",
    "\n",
    "        print(f\"Scanning DLQ for matching MessageIds: {message_ids_to_redrive}\")\n",
    "        while True:\n",
    "            response = sqs.receive_message(\n",
    "                QueueUrl=queue_url,\n",
    "                MaxNumberOfMessages=10,\n",
    "                VisibilityTimeout=30,\n",
    "                WaitTimeSeconds=10\n",
    "            )\n",
    "            messages = response.get('Messages', [])\n",
    "            if not messages:\n",
    "                print(\"No more messages available in the queue.\")\n",
    "                break\n",
    "            for message in messages:\n",
    "                message_id = message.get('MessageId', '')\n",
    "                receipt_handle = message.get('ReceiptHandle', '')\n",
    "                body = message.get('Body', '{}')\n",
    "                if message_id in message_ids_to_redrive:\n",
    "                    matched_messages.append({\n",
    "                        'MessageId': message_id,\n",
    "                        'ReceiptHandle': receipt_handle,\n",
    "                        'Body': body\n",
    "                    })\n",
    "                fetched_messages += 1\n",
    "                if fetched_messages >= MAX_MESSAGES:\n",
    "                    print(f\"Reached maximum limit of {MAX_MESSAGES} messages.\")\n",
    "                    break\n",
    "            if fetched_messages >= MAX_MESSAGES:\n",
    "                break\n",
    "        if not matched_messages:\n",
    "            print(\"No matching messages found in the DLQ.\")\n",
    "            return\n",
    "        print(f\"Found {len(matched_messages)} matching messages:\")\n",
    "        for msg in matched_messages:\n",
    "            print(f\"MessageId: {msg['MessageId']}, Body: {msg['Body']}\")\n",
    "        should_redrive = input(\"Redrive these messages to the original queue? (y/n): \")\n",
    "        if should_redrive.lower() != 'y':\n",
    "            print(\"Exiting without redriving.\")\n",
    "            return\n",
    "        for msg in matched_messages:\n",
    "            # Send to original queue\n",
    "            sqs.send_message(\n",
    "                QueueUrl=orig_queue_url,\n",
    "                MessageBody=msg['Body']\n",
    "            )\n",
    "            print(f\"Redriven MessageId: {msg['MessageId']} to {orig_queue_name}\")\n",
    "            # Optionally, delete from DLQ\n",
    "            delete_from_dlq = input(f\"Delete MessageId {msg['MessageId']} from DLQ? (y/n): \")\n",
    "            if delete_from_dlq.lower() == 'y':\n",
    "                sqs.delete_message(\n",
    "                    QueueUrl=queue_url,\n",
    "                    ReceiptHandle=msg['ReceiptHandle']\n",
    "                )\n",
    "                print(f\"Deleted MessageId: {msg['MessageId']} from DLQ.\")\n",
    "        print(\"Done.\")\n",
    "    except NoCredentialsError:\n",
    "        print(\"Error: AWS credentials not found. Please authenticate using ktool.\")\n",
    "    except PartialCredentialsError:\n",
    "        print(\"Error: Incomplete AWS credentials. Please check your ktool configuration.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Description: Script to get messages from the clustering DLQ and move it to the beginning of the process by sending an event to EventBridge.\n",
    "Why would I do this? \n",
    "Usually just redrive the DLQ and starting from the clustering step shoudl be good enough, but we've had situations where the messages in the DLQ are so old\n",
    "that their associated files in S3 have been deleted. \n",
    "In that case, we can't just redrive the DLQ, we need to start from the beginning of the process.\n",
    "\n",
    "\n",
    "🚨 WARNING: PLEASE USE THIS CAREFULLY. \n",
    "Read through the script and make necessary adjustments before using. \n",
    "We are querying the Database directly, so we want to be careful with that. Additionally, this script can delete messages from SQS - \n",
    "and it's a permenant delete, so I've added an input to make sure that the user wants to delete the message.\n",
    "IF you have a lot of messages, and are confident that the script is doing what you need, then you can comment out those input logs. \n",
    "\n",
    "How to use?\n",
    "1. Replace your DB user name and password, port in the psycopg.connect() method.\n",
    "2. Run the script.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "import psycopg\n",
    "from psycopg import sql\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Make sure the user wants to proceed\n",
    "    are_you_sure = input('Running this script will send events to EventBridge and delete messages from the SQS queue. Are you sure you want to proceed? (y/n): ')\n",
    "    if are_you_sure.lower() != 'y':\n",
    "        print(\"Exiting script.\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    try:\n",
    "        # Initialize a session using the default credentials (managed by ktool)\n",
    "        session = boto3.Session(\n",
    "            region_name='us-west-2',  # Replace with your AWS region\n",
    "            profile_name='lattice'\n",
    "        )\n",
    "\n",
    "        # Create SQS and EventBridge clients using the session\n",
    "        sqs = session.client('sqs')\n",
    "        eventbridge = session.client('events')\n",
    "\n",
    "        # Get the URL for the SQS queue\n",
    "        queue_url = sqs.get_queue_url(QueueName='text-analysis-clustering-dlq')['QueueUrl']\n",
    "\n",
    "        all_messages = []\n",
    "\n",
    "        # Define maximum number of messages to prevent infinite loops (optional)\n",
    "        MAX_MESSAGES = 100  # Adjust as needed\n",
    "        fetched_messages = 0\n",
    "\n",
    "        print(\"Starting to create database connection\")\n",
    "        # Establish a single database connection outside the loop\n",
    "        with psycopg.connect('host=cyral-sidecar.lattice.com port=<port_number> dbname=entitystore user=user.name@lattice.com password=<cyral_password_here> sslmode=require') as conn:\n",
    "            \n",
    "            print(\"Database connection established\")\n",
    "            with conn.cursor() as cur:\n",
    "                print(\"Database cursor created\")\n",
    "                while True:\n",
    "                    # Receive messages from SQS queue\n",
    "                    response = sqs.receive_message(\n",
    "                        QueueUrl=queue_url,\n",
    "                        MaxNumberOfMessages=10,  # Increase to fetch more messages per request\n",
    "                        VisibilityTimeout=30,    # Adjust based on processing time\n",
    "                        WaitTimeSeconds=10       # Long polling\n",
    "                    )\n",
    "\n",
    "                    messages = response.get('Messages', [])\n",
    "\n",
    "                    if not messages:\n",
    "                        print(\"No more messages available in the queue.\")\n",
    "                        break\n",
    "\n",
    "                    for message in messages:\n",
    "                        message_id = message.get('MessageId', '')\n",
    "                        receipt_handle = message.get('ReceiptHandle', '')\n",
    "                        body = message.get('Body', '{}')  # Default to empty JSON if Body is missing\n",
    "\n",
    "                        try:\n",
    "                            # Parse the JSON body\n",
    "                            body_json = json.loads(body)\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error decoding JSON for MessageId {message_id}: {e}\")\n",
    "                            body_json = {}\n",
    "\n",
    "                        # Parse out the entity id from the body\n",
    "                        # Example path: 'standard/performanceSummaries/hierarchical/24b0e6ef-25e6-4a24-b72a-7ff6e5783a7e.csv'\n",
    "                        record = body_json.get('Records', [{}])[0]\n",
    "                        s3_key = record.get('s3', {}).get('object', {}).get('key', '')\n",
    "\n",
    "                        if not s3_key:\n",
    "                            print(f\"Skipping message {message_id} with no object key.\")\n",
    "                            continue\n",
    "\n",
    "                        # Extract the entity_id by taking the last part after '/' and removing '.csv'\n",
    "                        last_part = s3_key.rsplit('/', 1)[-1]\n",
    "                        entity_id = last_part.rsplit('.', 1)[0]\n",
    "\n",
    "                        print(f\"Processing MessageId: {message_id}, Entity ID: {entity_id}\")\n",
    "\n",
    "                        try:\n",
    "                            # Execute the SQL query securely\n",
    "                            query = sql.SQL(\"SELECT * FROM text_analysis.summary_analyses WHERE entity_id = %s\")\n",
    "                            cur.execute(query, (entity_id,))\n",
    "                            summary_analysis = cur.fetchone()\n",
    "                            # Get column names from cursor.description\n",
    "                            columns = [desc[0] for desc in cur.description]\n",
    "                            # Create a dictionary mapping column names to their respective values\n",
    "                            summary_analysis_dict = dict(zip(columns, summary_analysis))\n",
    "\n",
    "                            print(f\"Summary Analysis for Entity ID {entity_id}: {summary_analysis}\")\n",
    "                            company_entity_id = summary_analysis_dict.get('company_entity_id')\n",
    "                            if company_entity_id is not None:\n",
    "                                company_entity_id = str(company_entity_id)\n",
    "                            product_surface = summary_analysis_dict.get('product_surface')\n",
    "                            target_entity_id = summary_analysis_dict.get('target_entity_id')\n",
    "                            if target_entity_id is not None: \n",
    "                                target_entity_id = str(target_entity_id)\n",
    "                            target_entity_type = summary_analysis_dict.get('target_entity_type')\n",
    "                            target_key = summary_analysis_dict.get('target_key')\n",
    "\n",
    "                            event_detail = {\n",
    "                                'summaryAnalysisEntityId': entity_id,\n",
    "                                'companyEntityId': company_entity_id,\n",
    "                                'productSurface': product_surface,\n",
    "                                'targetEntityId': target_entity_id,\n",
    "                                'targetEntityType': target_entity_type,\n",
    "                                'targetKey': target_key,\n",
    "                            }\n",
    "                            print(event_detail)\n",
    "                            print(json.dumps(event_detail))\n",
    "                        except psycopg.Error as db_err:\n",
    "                            print(f\"Database error for Entity ID {entity_id}: {db_err}\")\n",
    "                            # Optionally, continue to next message or handle as needed\n",
    "                            continue\n",
    "\n",
    "\n",
    "                        # get user input if we should send the event to eventbridge (comment out the next line for faster execution)\n",
    "                        send_event = input(\"Send event to EventBridge? (y/n): \")\n",
    "                        if send_event.lower() != 'y':\n",
    "                            print(f\"Skipping sending event for Entity ID {entity_id}.\")\n",
    "                            continue\n",
    "                        # Send message to EventBridge to start the process again\n",
    "                        try:\n",
    "                            eventbridge_response = eventbridge.put_events(\n",
    "                                Entries=[\n",
    "                                    {\n",
    "                                        'Source': 'lattice.custom.text-analysis',\n",
    "                                        'DetailType': 'lattice.ai-platform.summary-analysis.generation-triggered',\n",
    "                                        'Detail': json.dumps(event_detail),\n",
    "                                        'EventBusName': 'lattice-event-bus',\n",
    "                                    }\n",
    "                                ]\n",
    "                            )\n",
    "                            print(f\"EventBridge response: {eventbridge_response}\")\n",
    "                            \n",
    "                            print(\"failed entry count: \", eventbridge_response.get('FailedEntryCount'))\n",
    "                            print(f\"Event sent to EventBridge for Entity ID {entity_id}.\")\n",
    "\n",
    "                            if(eventbridge_response.get('FailedEntryCount') == 0):\n",
    "                              \n",
    "                                try:\n",
    "                                     # Optional: Delete the message after successful processing  (comment out the next line for faster execution)\n",
    "                                    delete_msg = input(\"Delete message from SQS? (y/n): \")\n",
    "                                    if delete_msg.lower() != 'y': \n",
    "                                        print(f\"Skipping deletion of message {message_id}.\")\n",
    "                                        continue\n",
    "                                    sqs.delete_message(\n",
    "                                        QueueUrl=queue_url,\n",
    "                                        ReceiptHandle=receipt_handle\n",
    "                                    )\n",
    "                                    print(f\"Deleted MessageId: {message_id} from SQS queue.\")\n",
    "                                except Exception as del_err:\n",
    "                                    print(f\"Failed to delete MessageId {message_id}: {del_err}\")\n",
    "                            else:\n",
    "                                print(\"Error sending to EventBridge\")\n",
    "                                # Optionally, handle the error\n",
    "                           \n",
    "                          \n",
    "                        except Exception as eb_err:\n",
    "                            print(f\"Failed to send event for Entity ID {entity_id}: {eb_err}\")\n",
    "                            \n",
    "                        fetched_messages += 1\n",
    "                        print(\"Fetched messages: \", fetched_messages)\n",
    "                        # Check if maximum message limit is reached\n",
    "                        if fetched_messages >= MAX_MESSAGES:\n",
    "                            print(f\"Reached maximum limit of {MAX_MESSAGES} messages.\")\n",
    "                            break\n",
    "\n",
    "                    # Optional: Print progress\n",
    "                    print(f\"Fetched {fetched_messages} messages so far.\")\n",
    "\n",
    "                    # Exit if maximum message limit is reached\n",
    "                    if fetched_messages >= MAX_MESSAGES:\n",
    "                        break\n",
    "\n",
    "        if not all_messages:\n",
    "            print(\"No messages were fetched from the queue.\")\n",
    "            return\n",
    "\n",
    "    except NoCredentialsError:\n",
    "        print(\"Error: AWS credentials not found. Please authenticate using ktool.\")\n",
    "    except PartialCredentialsError:\n",
    "        print(\"Error: Incomplete AWS credentials. Please check your ktool configuration.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
