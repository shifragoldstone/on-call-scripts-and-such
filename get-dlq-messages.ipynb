{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run the instructions in here - [Accessing AWS](https://www.notion.so/lattice/Accessing-AWS-043c558c833044e38329ef3dc9c1ea26#4e7d707777864670814e37aef0a08a6b) which will log you in using your okta roles. this will last 4 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 10 messages so far.\n",
      "Fetched 20 messages so far.\n",
      "Fetched 30 messages so far.\n",
      "Fetched 40 messages so far.\n",
      "Fetched 50 messages so far.\n",
      "Fetched 60 messages so far.\n",
      "Fetched 70 messages so far.\n",
      "Fetched 80 messages so far.\n",
      "Fetched 90 messages so far.\n",
      "Fetched 100 messages so far.\n",
      "Fetched 110 messages so far.\n",
      "Fetched 120 messages so far.\n",
      "Fetched 130 messages so far.\n",
      "Fetched 140 messages so far.\n",
      "Fetched 150 messages so far.\n",
      "Fetched 160 messages so far.\n",
      "Fetched 170 messages so far.\n",
      "Fetched 180 messages so far.\n",
      "Fetched 190 messages so far.\n",
      "Fetched 200 messages so far.\n",
      "Fetched 210 messages so far.\n",
      "Fetched 220 messages so far.\n",
      "Fetched 230 messages so far.\n",
      "Fetched 240 messages so far.\n",
      "Fetched 250 messages so far.\n",
      "Fetched 260 messages so far.\n",
      "Fetched 270 messages so far.\n",
      "Fetched 280 messages so far.\n",
      "Fetched 290 messages so far.\n",
      "Fetched 300 messages so far.\n",
      "Fetched 310 messages so far.\n",
      "Fetched 320 messages so far.\n",
      "Fetched 330 messages so far.\n",
      "Fetched 340 messages so far.\n",
      "Fetched 350 messages so far.\n",
      "Fetched 360 messages so far.\n",
      "Fetched 370 messages so far.\n",
      "Fetched 380 messages so far.\n",
      "Fetched 390 messages so far.\n",
      "Fetched 400 messages so far.\n",
      "Fetched 410 messages so far.\n",
      "Fetched 420 messages so far.\n",
      "Fetched 430 messages so far.\n",
      "Fetched 440 messages so far.\n",
      "Fetched 450 messages so far.\n",
      "Fetched 460 messages so far.\n",
      "Fetched 470 messages so far.\n",
      "Fetched 480 messages so far.\n",
      "Fetched 490 messages so far.\n",
      "Fetched 500 messages so far.\n",
      "Fetched 510 messages so far.\n",
      "Fetched 520 messages so far.\n",
      "Fetched 530 messages so far.\n",
      "Fetched 540 messages so far.\n",
      "Fetched 550 messages so far.\n",
      "Fetched 560 messages so far.\n",
      "Fetched 570 messages so far.\n",
      "Fetched 580 messages so far.\n",
      "Fetched 590 messages so far.\n",
      "Fetched 600 messages so far.\n",
      "Fetched 610 messages so far.\n",
      "Fetched 620 messages so far.\n",
      "Fetched 630 messages so far.\n",
      "Fetched 640 messages so far.\n",
      "Fetched 650 messages so far.\n",
      "Fetched 660 messages so far.\n",
      "Fetched 670 messages so far.\n",
      "Fetched 680 messages so far.\n",
      "Fetched 690 messages so far.\n",
      "Fetched 700 messages so far.\n",
      "Fetched 710 messages so far.\n",
      "Fetched 720 messages so far.\n",
      "Fetched 730 messages so far.\n",
      "Fetched 740 messages so far.\n",
      "Fetched 750 messages so far.\n",
      "Fetched 760 messages so far.\n",
      "Fetched 770 messages so far.\n",
      "Fetched 775 messages so far.\n",
      "Fetched 785 messages so far.\n",
      "Fetched 795 messages so far.\n",
      "Fetched 805 messages so far.\n",
      "Fetched 815 messages so far.\n",
      "Fetched 825 messages so far.\n",
      "Fetched 835 messages so far.\n",
      "Fetched 845 messages so far.\n",
      "Fetched 855 messages so far.\n",
      "Fetched 865 messages so far.\n",
      "Fetched 875 messages so far.\n",
      "Fetched 885 messages so far.\n",
      "Fetched 895 messages so far.\n",
      "Fetched 905 messages so far.\n",
      "Fetched 915 messages so far.\n",
      "Fetched 925 messages so far.\n",
      "Fetched 935 messages so far.\n",
      "Fetched 945 messages so far.\n",
      "Fetched 955 messages so far.\n",
      "Fetched 965 messages so far.\n",
      "Fetched 975 messages so far.\n",
      "Fetched 985 messages so far.\n",
      "Fetched 995 messages so far.\n",
      "Fetched 1002 messages so far.\n",
      "Fetched 1004 messages so far.\n",
      "Fetched 1011 messages so far.\n",
      "Fetched 1021 messages so far.\n",
      "Fetched 1031 messages so far.\n",
      "Fetched 1041 messages so far.\n",
      "Fetched 1051 messages so far.\n",
      "Fetched 1061 messages so far.\n",
      "Fetched 1071 messages so far.\n",
      "Fetched 1081 messages so far.\n",
      "Fetched 1091 messages so far.\n",
      "Fetched 1101 messages so far.\n",
      "Fetched 1111 messages so far.\n",
      "Fetched 1115 messages so far.\n",
      "Fetched 1120 messages so far.\n",
      "No more messages available in the queue.\n",
      "1120 messages have been written to sqs_messages.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SCRIPT TO GET MESSAGES FROM QUEUE AND WRITE TO CSV\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import csv\n",
    "import json\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "\n",
    "def flatten_json(y, parent_key='', sep='.'):\n",
    "    \"\"\"\n",
    "    Flattens a nested JSON object.\n",
    "\n",
    "    :param y: The JSON object to flatten.\n",
    "    :param parent_key: The base key string.\n",
    "    :param sep: Separator between keys.\n",
    "    :return: A flattened dictionary.\n",
    "    \"\"\"\n",
    "    items = {}\n",
    "    for k, v in y.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.update(flatten_json(v, new_key, sep=sep))\n",
    "        elif isinstance(v, list):\n",
    "            # Handle lists by joining items with a separator or indexing\n",
    "            items[new_key] = '; '.join(map(str, v))\n",
    "        else:\n",
    "            items[new_key] = v\n",
    "    return items\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize a session using the default credentials (managed by ktool)\n",
    "        session = boto3.Session(\n",
    "            region_name='us-west-2',  # Replace with your AWS region\n",
    "            profile_name='lattice'\n",
    "        )\n",
    "\n",
    "        # Create SQS client using the session\n",
    "        sqs = session.client('sqs')\n",
    "\n",
    "        # Get the URL for the SQS queue\n",
    "        # queue_url = sqs.get_queue_url(QueueName='weaver-worker-text-analysis-dlq')['QueueUrl']\n",
    "        queue_url = sqs.get_queue_url(QueueName='text-analysis-clustering-dlq')['QueueUrl']\n",
    "\n",
    "        all_messages = []\n",
    "        csv_headers = set(['MessageId', 'ReceiptHandle'])  # Initial headers\n",
    "\n",
    "        # Define maximum number of messages to prevent infinite loops (optional)\n",
    "        MAX_MESSAGES = 10000  # Adjust as needed\n",
    "        fetched_messages = 0\n",
    "\n",
    "        while True:\n",
    "            # Receive messages from SQS queue\n",
    "            response = sqs.receive_message(\n",
    "                QueueUrl=queue_url,\n",
    "                MaxNumberOfMessages=10,  # Maximum allowed by SQS\n",
    "                VisibilityTimeout=30,    # Adjust based on processing time\n",
    "                WaitTimeSeconds=10       # Long polling\n",
    "            )\n",
    "\n",
    "            messages = response.get('Messages', [])\n",
    "\n",
    "            if not messages:\n",
    "                print(\"No more messages available in the queue.\")\n",
    "                break\n",
    "\n",
    "            for message in messages:\n",
    "                message_id = message.get('MessageId', '')\n",
    "                receipt_handle = message.get('ReceiptHandle', '')\n",
    "                body = message.get('Body', '{}')  # Default to empty JSON if Body is missing\n",
    "\n",
    "                try:\n",
    "                    # Parse the JSON body\n",
    "                    body_json = json.loads(body)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON for MessageId {message_id}: {e}\")\n",
    "                    body_json = {}\n",
    "\n",
    "                # Flatten the JSON body\n",
    "                flattened_body = flatten_json(body_json)\n",
    "\n",
    "                # Update headers with keys from the flattened JSON\n",
    "                csv_headers.update(flattened_body.keys())\n",
    "\n",
    "                # Combine message metadata with flattened body\n",
    "                combined_message = {\n",
    "                    'MessageId': message_id,\n",
    "                    'ReceiptHandle': receipt_handle\n",
    "                }\n",
    "                combined_message.update(flattened_body)\n",
    "\n",
    "                all_messages.append(combined_message)\n",
    "\n",
    "                # Optional: Delete the message after processing\n",
    "                # Uncomment the following lines if you want to delete messages\n",
    "                # sqs.delete_message(\n",
    "                #     QueueUrl=queue_url,\n",
    "                #     ReceiptHandle=receipt_handle\n",
    "                # )\n",
    "\n",
    "                fetched_messages += 1\n",
    "\n",
    "                # Check if maximum message limit is reached\n",
    "                if fetched_messages >= MAX_MESSAGES:\n",
    "                    print(f\"Reached maximum limit of {MAX_MESSAGES} messages.\")\n",
    "                    break\n",
    "\n",
    "            # Optional: Print progress\n",
    "            print(f\"Fetched {fetched_messages} messages so far.\")\n",
    "\n",
    "            # Exit if maximum message limit is reached\n",
    "            if fetched_messages >= MAX_MESSAGES:\n",
    "                break\n",
    "\n",
    "        if not all_messages:\n",
    "            print(\"No messages were fetched from the queue.\")\n",
    "            return\n",
    "\n",
    "        # Define the order of CSV headers (optional)\n",
    "        ordered_headers = ['MessageId', 'ReceiptHandle'] + sorted(k for k in csv_headers if k not in ['MessageId', 'ReceiptHandle'])\n",
    "\n",
    "        # Write CSV data to a file\n",
    "        with open('sqs_messages.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=ordered_headers)\n",
    "            writer.writeheader()\n",
    "            for msg in all_messages:\n",
    "                writer.writerow(msg)\n",
    "\n",
    "        print(f\"{len(all_messages)} messages have been written to sqs_messages.csv\")\n",
    "\n",
    "    except NoCredentialsError:\n",
    "        print(\"Error: AWS credentials not found. Please authenticate using ktool.\")\n",
    "    except PartialCredentialsError:\n",
    "        print(\"Error: Incomplete AWS credentials. Please check your ktool configuration.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to create database connection\n",
      "Database connection established\n",
      "Database cursor created\n",
      "Processing MessageId: fb645193-a47d-4d1c-9a5f-3b0021dbaf4a, Entity ID: a481fed7-a6d5-4cbe-8c52-706ad49fdb03\n",
      "Summary Analysis for Entity ID a481fed7-a6d5-4cbe-8c52-706ad49fdb03: (UUID('a481fed7-a6d5-4cbe-8c52-706ad49fdb03'), datetime.datetime(2024, 12, 12, 15, 42, 56, 141000, tzinfo=datetime.timezone.utc), datetime.datetime(2025, 1, 2, 14, 46, 29, 68000, tzinfo=datetime.timezone.utc), UUID('1c541048-8a3e-11ef-bbb6-df7ba7312a77'), 'complete', 'performanceSummaries12Month', 'reviewee', UUID('c2ae037f-a8d5-43a9-9ce6-78d59b913da5'), None)\n",
      "c2ae037f-a8d5-43a9-9ce6-78d59b913da5\n",
      "performanceSummaries12Month\n",
      "{'summaryAnalysisEntityId': 'a481fed7-a6d5-4cbe-8c52-706ad49fdb03', 'companyEntityId': 'c2ae037f-a8d5-43a9-9ce6-78d59b913da5', 'productSurface': 'performanceSummaries12Month', 'targetEntityId': '1c541048-8a3e-11ef-bbb6-df7ba7312a77', 'targetEntityType': 'reviewee', 'targetKey': None}\n",
      "{\"summaryAnalysisEntityId\": \"a481fed7-a6d5-4cbe-8c52-706ad49fdb03\", \"companyEntityId\": \"c2ae037f-a8d5-43a9-9ce6-78d59b913da5\", \"productSurface\": \"performanceSummaries12Month\", \"targetEntityId\": \"1c541048-8a3e-11ef-bbb6-df7ba7312a77\", \"targetEntityType\": \"reviewee\", \"targetKey\": null}\n",
      "EventBridge response: {'FailedEntryCount': 0, 'Entries': [{'EventId': '517587bb-3280-1f0c-6e07-5186b928351d'}], 'ResponseMetadata': {'RequestId': 'd3e93512-be60-42f2-a78a-cf88bad40abb', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'd3e93512-be60-42f2-a78a-cf88bad40abb', 'content-type': 'application/x-amz-json-1.1', 'content-length': '85', 'date': 'Thu, 02 Jan 2025 14:51:13 GMT'}, 'RetryAttempts': 0}}\n",
      "failed entry count:  0\n",
      "Event sent to EventBridge for Entity ID a481fed7-a6d5-4cbe-8c52-706ad49fdb03.\n",
      "All good, deleting from SQS\n",
      "Deleted MessageId: fb645193-a47d-4d1c-9a5f-3b0021dbaf4a from SQS queue.\n",
      "Fetched messages:  1\n",
      "Processing MessageId: 4226865e-43be-4818-b445-7042ccf15162, Entity ID: f25b3b30-8e80-40ac-b812-d247de5c2af4\n",
      "Summary Analysis for Entity ID f25b3b30-8e80-40ac-b812-d247de5c2af4: (UUID('f25b3b30-8e80-40ac-b812-d247de5c2af4'), datetime.datetime(2024, 12, 11, 20, 24, 11, 202000, tzinfo=datetime.timezone.utc), datetime.datetime(2024, 12, 11, 20, 24, 11, 202000, tzinfo=datetime.timezone.utc), UUID('dbfbdc72-84c8-11ee-8dfc-5b165abde65a'), 'in_progress', 'performanceSummaries', 'reviewee', UUID('53594ff9-ada9-47c1-9ca4-d8fa199786de'), None)\n",
      "53594ff9-ada9-47c1-9ca4-d8fa199786de\n",
      "performanceSummaries\n",
      "{'summaryAnalysisEntityId': 'f25b3b30-8e80-40ac-b812-d247de5c2af4', 'companyEntityId': '53594ff9-ada9-47c1-9ca4-d8fa199786de', 'productSurface': 'performanceSummaries', 'targetEntityId': 'dbfbdc72-84c8-11ee-8dfc-5b165abde65a', 'targetEntityType': 'reviewee', 'targetKey': None}\n",
      "{\"summaryAnalysisEntityId\": \"f25b3b30-8e80-40ac-b812-d247de5c2af4\", \"companyEntityId\": \"53594ff9-ada9-47c1-9ca4-d8fa199786de\", \"productSurface\": \"performanceSummaries\", \"targetEntityId\": \"dbfbdc72-84c8-11ee-8dfc-5b165abde65a\", \"targetEntityType\": \"reviewee\", \"targetKey\": null}\n",
      "EventBridge response: {'FailedEntryCount': 0, 'Entries': [{'EventId': '1bc69899-a2bd-a4c3-c467-5c2262b78e29'}], 'ResponseMetadata': {'RequestId': 'a5f243f9-6e35-490a-9cd3-bbe3ef01c427', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'a5f243f9-6e35-490a-9cd3-bbe3ef01c427', 'content-type': 'application/x-amz-json-1.1', 'content-length': '85', 'date': 'Thu, 02 Jan 2025 14:51:13 GMT'}, 'RetryAttempts': 0}}\n",
      "failed entry count:  0\n",
      "Event sent to EventBridge for Entity ID f25b3b30-8e80-40ac-b812-d247de5c2af4.\n",
      "All good, deleting from SQS\n",
      "Deleted MessageId: 4226865e-43be-4818-b445-7042ccf15162 from SQS queue.\n",
      "Fetched messages:  2\n",
      "Fetched 2 messages so far.\n",
      "No more messages available in the queue.\n",
      "No messages were fetched from the queue.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Script to get messages from the clustering DLQ and move it to the beginning of the process by sending an event to EventBridge.\n",
    "Why would I do this? \n",
    "Usually just redrive the DLQ and starting from the clustering step shoudl be good enough, but we've had situations where the messages in the DLQ are so old\n",
    "that their associated files in S3 have been deleted. \n",
    "In that case, we can't just redrive the DLQ, we need to start from the beginning of the process.\n",
    "\n",
    "\n",
    "🚨 WARNING: PLEASE USE THIS CAREFULLY. \n",
    "Read through the script and make necessary adjustments before using. \n",
    "We are querying the Database directly, so we want to be careful with that. Additionally, this script can delete messages from SQS - \n",
    "and it's a permenant delete, so I've added an input to make sure that the user wants to delete the message.\n",
    "IF you have a lot of messages, and are confident that the script is doing what you need, then you can comment out those input logs. \n",
    "\n",
    "How to use?\n",
    "1. Replace your DB user name and password in the psycopg.connect() method.\n",
    "2. Run the script.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "import psycopg\n",
    "from psycopg import sql\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Make sure the user wants to proceed\n",
    "    are_you_sure = input('Running this script will send events to EventBridge and delete messages from the SQS queue. Are you sure you want to proceed? (y/n): ')\n",
    "    if are_you_sure.lower() != 'y':\n",
    "        print(\"Exiting script.\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    try:\n",
    "        # Initialize a session using the default credentials (managed by ktool)\n",
    "        session = boto3.Session(\n",
    "            region_name='us-west-2',  # Replace with your AWS region\n",
    "            profile_name='lattice'\n",
    "        )\n",
    "\n",
    "        # Create SQS and EventBridge clients using the session\n",
    "        sqs = session.client('sqs')\n",
    "        eventbridge = session.client('events')\n",
    "\n",
    "        # Get the URL for the SQS queue\n",
    "        queue_url = sqs.get_queue_url(QueueName='text-analysis-clustering-dlq')['QueueUrl']\n",
    "\n",
    "        all_messages = []\n",
    "\n",
    "        # Define maximum number of messages to prevent infinite loops (optional)\n",
    "        MAX_MESSAGES = 100  # Adjust as needed\n",
    "        fetched_messages = 0\n",
    "\n",
    "        print(\"Starting to create database connection\")\n",
    "        # Establish a single database connection outside the loop\n",
    "        with psycopg.connect('host=cyral-sidecar.lattice.com port=5432 dbname=entitystore user=user.name@lattice.com password=<cyral_password_here> sslmode=require') as conn:\n",
    "            \n",
    "            print(\"Database connection established\")\n",
    "            with conn.cursor() as cur:\n",
    "                print(\"Database cursor created\")\n",
    "                while True:\n",
    "                    # Receive messages from SQS queue\n",
    "                    response = sqs.receive_message(\n",
    "                        QueueUrl=queue_url,\n",
    "                        MaxNumberOfMessages=10,  # Increase to fetch more messages per request\n",
    "                        VisibilityTimeout=30,    # Adjust based on processing time\n",
    "                        WaitTimeSeconds=10       # Long polling\n",
    "                    )\n",
    "\n",
    "                    messages = response.get('Messages', [])\n",
    "\n",
    "                    if not messages:\n",
    "                        print(\"No more messages available in the queue.\")\n",
    "                        break\n",
    "\n",
    "                    for message in messages:\n",
    "                        message_id = message.get('MessageId', '')\n",
    "                        receipt_handle = message.get('ReceiptHandle', '')\n",
    "                        body = message.get('Body', '{}')  # Default to empty JSON if Body is missing\n",
    "\n",
    "                        try:\n",
    "                            # Parse the JSON body\n",
    "                            body_json = json.loads(body)\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error decoding JSON for MessageId {message_id}: {e}\")\n",
    "                            body_json = {}\n",
    "\n",
    "                        # Parse out the entity id from the body\n",
    "                        # Example path: 'standard/performanceSummaries/hierarchical/24b0e6ef-25e6-4a24-b72a-7ff6e5783a7e.csv'\n",
    "                        record = body_json.get('Records', [{}])[0]\n",
    "                        s3_key = record.get('s3', {}).get('object', {}).get('key', '')\n",
    "\n",
    "                        if not s3_key:\n",
    "                            print(f\"Skipping message {message_id} with no object key.\")\n",
    "                            continue\n",
    "\n",
    "                        # Extract the entity_id by taking the last part after '/' and removing '.csv'\n",
    "                        last_part = s3_key.rsplit('/', 1)[-1]\n",
    "                        entity_id = last_part.rsplit('.', 1)[0]\n",
    "\n",
    "                        print(f\"Processing MessageId: {message_id}, Entity ID: {entity_id}\")\n",
    "\n",
    "                        try:\n",
    "                            # Execute the SQL query securely\n",
    "                            query = sql.SQL(\"SELECT * FROM text_analysis.summary_analyses WHERE entity_id = %s\")\n",
    "                            cur.execute(query, (entity_id,))\n",
    "                            summary_analysis = cur.fetchone()\n",
    "                            # Get column names from cursor.description\n",
    "                            columns = [desc[0] for desc in cur.description]\n",
    "                            # Create a dictionary mapping column names to their respective values\n",
    "                            summary_analysis_dict = dict(zip(columns, summary_analysis))\n",
    "\n",
    "                            print(f\"Summary Analysis for Entity ID {entity_id}: {summary_analysis}\")\n",
    "                            company_entity_id = summary_analysis_dict.get('company_entity_id')\n",
    "                            if company_entity_id is not None:\n",
    "                                company_entity_id = str(company_entity_id)\n",
    "                            product_surface = summary_analysis_dict.get('product_surface')\n",
    "                            target_entity_id = summary_analysis_dict.get('target_entity_id')\n",
    "                            if target_entity_id is not None: \n",
    "                                target_entity_id = str(target_entity_id)\n",
    "                            target_entity_type = summary_analysis_dict.get('target_entity_type')\n",
    "                            target_key = summary_analysis_dict.get('target_key')\n",
    "\n",
    "                            event_detail = {\n",
    "                                'summaryAnalysisEntityId': entity_id,\n",
    "                                'companyEntityId': company_entity_id,\n",
    "                                'productSurface': product_surface,\n",
    "                                'targetEntityId': target_entity_id,\n",
    "                                'targetEntityType': target_entity_type,\n",
    "                                'targetKey': target_key,\n",
    "                            }\n",
    "                            print(event_detail)\n",
    "                            print(json.dumps(event_detail))\n",
    "                        except psycopg.Error as db_err:\n",
    "                            print(f\"Database error for Entity ID {entity_id}: {db_err}\")\n",
    "                            # Optionally, continue to next message or handle as needed\n",
    "                            continue\n",
    "\n",
    "\n",
    "                        # get user input if we should send the event to eventbridge (comment out the next line for faster execution)\n",
    "                        send_event = input(\"Send event to EventBridge? (y/n): \")\n",
    "                        if send_event.lower() != 'y':\n",
    "                            print(f\"Skipping sending event for Entity ID {entity_id}.\")\n",
    "                            continue\n",
    "                        # Send message to EventBridge to start the process again\n",
    "                        try:\n",
    "                            eventbridge_response = eventbridge.put_events(\n",
    "                                Entries=[\n",
    "                                    {\n",
    "                                        'Source': 'lattice.custom.text-analysis',\n",
    "                                        'DetailType': 'lattice.ai-platform.summary-analysis.generation-triggered',\n",
    "                                        'Detail': json.dumps(event_detail),\n",
    "                                        'EventBusName': 'lattice-event-bus',\n",
    "                                    }\n",
    "                                ]\n",
    "                            )\n",
    "                            print(f\"EventBridge response: {eventbridge_response}\")\n",
    "                            \n",
    "                            print(\"failed entry count: \", eventbridge_response.get('FailedEntryCount'))\n",
    "                            print(f\"Event sent to EventBridge for Entity ID {entity_id}.\")\n",
    "\n",
    "                            if(eventbridge_response.get('FailedEntryCount') == 0):\n",
    "                              \n",
    "                                try:\n",
    "                                     # Optional: Delete the message after successful processing  (comment out the next line for faster execution)\n",
    "                                    delete_msg = input(\"Delete message from SQS? (y/n): \")\n",
    "                                    if delete_msg.lower() != 'y': \n",
    "                                        print(f\"Skipping deletion of message {message_id}.\")\n",
    "                                        continue\n",
    "                                    sqs.delete_message(\n",
    "                                        QueueUrl=queue_url,\n",
    "                                        ReceiptHandle=receipt_handle\n",
    "                                    )\n",
    "                                    print(f\"Deleted MessageId: {message_id} from SQS queue.\")\n",
    "                                except Exception as del_err:\n",
    "                                    print(f\"Failed to delete MessageId {message_id}: {del_err}\")\n",
    "                            else:\n",
    "                                print(\"Error sending to EventBridge\")\n",
    "                                # Optionally, handle the error\n",
    "                           \n",
    "                          \n",
    "                        except Exception as eb_err:\n",
    "                            print(f\"Failed to send event for Entity ID {entity_id}: {eb_err}\")\n",
    "                            \n",
    "                        fetched_messages += 1\n",
    "                        print(\"Fetched messages: \", fetched_messages)\n",
    "                        # Check if maximum message limit is reached\n",
    "                        if fetched_messages >= MAX_MESSAGES:\n",
    "                            print(f\"Reached maximum limit of {MAX_MESSAGES} messages.\")\n",
    "                            break\n",
    "\n",
    "                    # Optional: Print progress\n",
    "                    print(f\"Fetched {fetched_messages} messages so far.\")\n",
    "\n",
    "                    # Exit if maximum message limit is reached\n",
    "                    if fetched_messages >= MAX_MESSAGES:\n",
    "                        break\n",
    "\n",
    "        if not all_messages:\n",
    "            print(\"No messages were fetched from the queue.\")\n",
    "            return\n",
    "\n",
    "    except NoCredentialsError:\n",
    "        print(\"Error: AWS credentials not found. Please authenticate using ktool.\")\n",
    "    except PartialCredentialsError:\n",
    "        print(\"Error: Incomplete AWS credentials. Please check your ktool configuration.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
